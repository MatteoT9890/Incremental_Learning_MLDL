{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ProjectIL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVwz03v-hDZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5tF7T4fYmPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotLossAccuracy(losses_tr,losses_val,train_accuracies,val_accuracies):\n",
        "    %matplotlib inline\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize = (8,8))\n",
        "    plt.plot(losses_tr,label='Training',marker='o',color='black')\n",
        "    plt.plot(losses_val,label='Validation',marker='^',color='grey')\n",
        "    plt.title(\"Loss vs Epochs\")\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.figure(figsize = (8,8))\n",
        "    plt.plot(train_accuracies,label = 'Training',marker='o',color='black')\n",
        "    plt.plot(val_accuracies,label='Validation',marker='^',color='grey')\n",
        "    plt.title(\"Accuracy vs Epochs\")\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "# plotLossAccuracy(losses_train[4:],losses_val[4:],train_accuracies,val_accuracies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiD0ATBIhCxK",
        "colab_type": "code",
        "outputId": "87beb83e-2fcb-49a8-bb28-e0efba00bc5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from  CIFAR100_dataset import MyCIFAR100\n",
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([\n",
        "                                      transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize( (0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))]) # Normalizes tensor with mean and standard deviation\n",
        "\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize( (0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "training_set = MyCIFAR100('/content',train=True, n_groups=10, transform=train_transform, download=True)\n",
        "test_set = MyCIFAR100('/content',train=False, n_groups=10, transform=eval_transform, download=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT612fYVvV9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'  \n",
        "\n",
        "BATCH_SIZE = 64     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch strain_dataloaderize, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 0.05         # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 1e-4  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 70             # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = [30,45,60]      # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1                 # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qNf6PdgypTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################################################################################################\n",
        "#  If joint == True loads all groups until 'group', otherwise it allocates just the group 'group' #\n",
        "###################################################################################################\n",
        "\n",
        "def create_dataloaders(training_set,test_set,group,BATCH_SIZE,joint=False):\n",
        "    if(joint == True):\n",
        "        train,val = training_set.get_train_val_joint(group)\n",
        "        test = test_set.get_groups_joint(group)\n",
        "    else:\n",
        "        train,val = training_set.get_train_val_group(group)\n",
        "        test = test_set.get_group(group)\n",
        "\n",
        "    train_dataloader =  DataLoader(train,batch_size=BATCH_SIZE,drop_last=True,num_workers=4,shuffle=True)\n",
        "    val_dataloader = DataLoader(val,batch_size=BATCH_SIZE,drop_last=False,num_workers=4)\n",
        "    test_dataloader = DataLoader(test,batch_size=BATCH_SIZE,drop_last=False,num_workers=4)\n",
        "\n",
        "    return train_dataloader,val_dataloader,test_dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtDOe2f23lDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from resnet import ResNet\n",
        "# from resnet import BasicBlock\n",
        "from torchvision.models import resnet18\n",
        "from resnet_cifar import resnet32\n",
        "\n",
        "def prepare_network(n_classes,pretrained=False):\n",
        "\n",
        "    # net = resnet34(pretrained=pretrained) # Loading resNet34 model    \n",
        "    # net.fc = nn.Linear(512, n_classes)\n",
        "    # net = ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "    # net.fc_layer = nn.Linear(256, 10)\n",
        "    # nn.init.xavier_uniform(net.fc_layer.weight)\n",
        "    # classifier = nn.Linear(net.out_dim, n_classes, bias=True)\n",
        "    # nn.init.kaiming_normal_(classifier.weight, nonlinearity=\"linear\")\n",
        "    # net.fc = classifier\n",
        "    # net.add_module('softmax',nn.Softmax())\n",
        "    net = resnet32()\n",
        "    net.linear = nn.Linear(64,n_classes)\n",
        "\n",
        "    return net\n",
        "\n",
        "def update_network(net,n_classes):\n",
        "    n_old_classes = net.linear.weight.shape[0]\n",
        "    prev_weights = torch.nn.Parameter(data = net.linear.weight,requires_grad=True)\n",
        "    net.linear = nn.Linear(64,n_classes)\n",
        "    net.linear.weight[0:n_old_classes] = prev_weights\n",
        "    return net\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmio36YLAYT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_training(net,lr,gamma,step_size):\n",
        "    # Define loss function\n",
        "    criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "    # criterion = nn.BCELoss(size_average=False) # for classification, we use Cross Entropy\n",
        "\n",
        "    # Choose parameters to optimize\n",
        "    # To access a different set of parameters, you have to access submodules of AlexNet\n",
        "    # (nn.Module objects, like AlexNet, implement the Composite Pattern)\n",
        "    # e.g.: parameters of the fully connected layers: net.classifier.parameters()\n",
        "    # e.g.: parameters of the convolutional layers: look at alexnet's source code ;)\n",
        "\n",
        "    # If the network is pre-trained, it is possible to freeze some layer\n",
        "\n",
        "    # net.layer1.requires_grad_ = False\n",
        "    # net.layer2._ = False\n",
        "    # net.layer3.requires_grad_ = False\n",
        "\n",
        "    parameters_to_optimize = net.parameters()\n",
        "\n",
        "    # Define optimizer\n",
        "    # An optimizer updates the weights based on loss\n",
        "    # We use SGD with momentum\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=lr, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "\n",
        "    # Define scheduler\n",
        "    # A scheduler dynamically changes learning rate\n",
        "    # The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, STEP_SIZE, gamma=gamma)\n",
        "\n",
        "    return (criterion,optimizer,scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s0_LrbdDduK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(val_dataloader,net):\n",
        "    running_corrects = 0\n",
        "    loss_val = []\n",
        "    for images, labels in val_dataloader:\n",
        "\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # Forward Pass\n",
        "        outputs = net(images)\n",
        "        # Get predictions\n",
        "        # sigm = nn.Sigmoid()\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        # for out,label in zip(outputs,labels):\n",
        "        #     # print(out)\n",
        "        #     loss = criterion(sigm(out),create_onehot(label,10))\n",
        "        #     loss_val.append(loss.item())\n",
        "        loss = criterion(outputs,labels)\n",
        "        loss_val.append(loss.item())\n",
        "        # Update Corrects\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(val_dataloader.dataset))\n",
        "    #print('Validation Accuracy: {}'.format(accuracy))\n",
        "    loss_val = np.array(loss_val).mean()\n",
        "    #print(\"Loss on the validation:\\t\",loss_val)\n",
        "\n",
        "    return accuracy,loss_val\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ5PGav-TiU-",
        "colab_type": "text"
      },
      "source": [
        "**Joint training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnwY55KhB8kW",
        "colab_type": "code",
        "outputId": "567f2037-0fce-4140-ac4a-c923ddf350fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "n_classes = 10\n",
        "n_groups = 10\n",
        "best_validation_loss = sys.maxsize\n",
        "net = prepare_network(n_classes=n_classes,pretrained=False)\n",
        "train_dataloader,val_dataloader,test_dataloader = create_dataloaders(training_set,test_set,1,BATCH_SIZE,joint=True)\n",
        "best_train_accuracies = []\n",
        "best_val_accuracies = []\n",
        "best_test_accuracies = []\n",
        "\n",
        "losses_train_all = []\n",
        "losses_validation_all = []\n",
        "\n",
        "for i in range(n_groups-1):\n",
        "\n",
        "    joint_step=i+1\n",
        "    print(\"STARTING JOINT TRAINING WITH GROUP:\\t\",joint_step)  \n",
        "    \n",
        "    if joint_step != 1:\n",
        "        net = update_network(best_net,n_classes + n_classes*(joint_step))\n",
        "        train_dataloader,val_dataloader,test_dataloader = create_dataloaders(training_set,test_set,joint_step+1,BATCH_SIZE,joint=True)\n",
        "    criterion,optimizer,scheduler = prepare_training(net,LR,GAMMA,STEP_SIZE)\n",
        "\n",
        "    losses_train = []\n",
        "    losses_val = []\n",
        "    val_accuracies = []\n",
        "    train_accuracies = []\n",
        "    current_step = 0\n",
        "\n",
        "    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "    # cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "    # Start iterating over the epochs\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        running_correct_train = 0\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr()))\n",
        "\n",
        "        # Iterate over the dataset\n",
        "        net.train() # Set Network to train mode\n",
        "        for images, labels in train_dataloader:\n",
        "            losses_tmp = []\n",
        "            \n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # PyTorch, by default, accumulates gradients after each backward pass\n",
        "            # We need to manually set the gradients to zero before starting a new iteration\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "            # Forward pass to the network\n",
        "            outputs = net(images)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            # print(preds)\n",
        "            # Compute loss based on output and ground truth\n",
        "            # sigm = nn.Sigmoid()\n",
        "            #for out,label in zip(outputs,labels):\n",
        "            # print(out)\n",
        "            # sigm_output = sigm(outputs)\n",
        "            # oh_matrix = one_hot_matrix(labels,10)\n",
        "            # print(sigm_output)\n",
        "            # print(oh_matrix)\n",
        "            # loss = sum(criterion(sigm_output[:,y],oh_matrix[:,y]) for y in labels)/len(labels)\n",
        "            loss = criterion(outputs, labels)\n",
        "            # print(loss.item())a\n",
        "            losses_tmp.append(loss.item())\n",
        "            \n",
        "            # preds = torch.tensor([training_set.labels_split[0][pred] for pred in list(preds)]).cuda()\n",
        "            running_correct_train += torch.sum(preds == labels.data).data.item()\n",
        "            # print(running_correct_train)\n",
        "\n",
        "            # Log loss\n",
        "            # if current_step % LOG_FREQUENCY == 0:\n",
        "            #     print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "\n",
        "        train_accuracy = running_correct_train/len(train_dataloader.dataset)\n",
        "        net.train(False)\n",
        "        val_accuracy,loss_val = validation(val_dataloader,net)\n",
        "        \n",
        "        if loss_val < best_validation_loss:\n",
        "            best_net = net\n",
        "            best_train_accuracy = train_accuracy\n",
        "            best_val_accuracy = val_accuracy\n",
        "        \n",
        "        # print(\"Total Augmented for this epoch : \",totAugmented)\n",
        "        #print(\"Accuracy on the training :\\t\",train_accuracy)\n",
        "        #print(\"Accuracy on the validation :\\t\",val_accuracy)\n",
        "\n",
        "        # Save some important values to plot\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        losses_train.append(np.array(losses_tmp).mean())\n",
        "        losses_val.append(loss_val)\n",
        "        # plotLossAccuracy(losses_train,losses_val,train_accuracies,val_accuracies)\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step() \n",
        "    \n",
        "    print(\"Accuracy on the training :\\t\",best_train_accuracy)\n",
        "    print(\"Accuracy on the validation :\\t\",best_val_accuracy)\n",
        "    losses_train_all.append(losses_train)\n",
        "    losses_validation_all.append(losses_val)\n",
        "    #Test on the train,val e test set\n",
        "    best_train_accuracies.append(best_train_accuracy)\n",
        "    best_val_accuracies.append(best_val_accuracy)\n",
        "    test_accuracy = validation(test_dataloader,best_net)[0]\n",
        "    print(\"Accuracy on the validation :\\t\",test_accuracy)\n",
        "    best_test_accuracies.append(test_accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STARTING JOINT TRAINING WITH GROUP:\t 1\n",
            "Starting epoch 1/70, LR = [0.05]\n",
            "Starting epoch 2/70, LR = [0.05]\n",
            "Starting epoch 3/70, LR = [0.05]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSxpry4y9eo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.linear.weight.is_leaf = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcuzcr8HyOj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E128U-qyF8Ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = nn.Sigmoid()\n",
        "loss = nn.BCELoss()\n",
        "input = torch.randn(3, requires_grad=True)\n",
        "target = torch.empty(3).random_(1)\n",
        "output = loss(m(input), target)\n",
        "output.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hWLJN8CZxMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_onehot(intLabel,num_classes):\n",
        "    onehot = torch.zeros(num_classes)\n",
        "    onehot[intLabel]=1\n",
        "    return onehot.cuda()\n",
        "\n",
        "def one_hot_matrix(labels,n_classes):\n",
        "    matrix = torch.zeros((len(labels),n_classes))\n",
        "    for index,y in enumerate(labels):\n",
        "        matrix[index] = create_onehot(y,n_classes)\n",
        "    return matrix.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}