{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTlj0v4qrzFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r 'IncrementalLeraningMLDL'\n",
        "!git clone \"https://github.com/wAnto97/IncrementalLeraningMLDL\"\n",
        "from IncrementalLeraningMLDL.src.CIFAR100_dataset import MyCIFAR100\n",
        "from IncrementalLeraningMLDL.src.Utils import Utils\n",
        "from IncrementalLeraningMLDL.src.MyNet import MyNet\n",
        "from IncrementalLeraningMLDL.src.Icarl import Icarl\n",
        "from IncrementalLeraningMLDL.src.Loss import Loss\n",
        "from IncrementalLeraningMLDL.src.Exemplars import Exemplars \n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "import copy\n",
        "from torch.backends import cudnn\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import  DataLoader\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jbSB5id_PC9",
        "colab_type": "text"
      },
      "source": [
        "**Loading data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NtjfRgDVse6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "                                      transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize( (0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))]) # Normalizes tensor with mean and standard deviation\n",
        "\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize( (0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "training_set = MyCIFAR100('/content',train=True, n_groups=10, transform=train_transform, download=True,random_state = 653) ###seed icarl 1993, nostro seed 653\n",
        "test_set = MyCIFAR100('/content',train=False, n_groups=10, transform=eval_transform, download=True,random_state = 653)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAub61CI_RQa",
        "colab_type": "text"
      },
      "source": [
        "**Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-do-BtKUI4F-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'  \n",
        "\n",
        "BATCH_SIZE = 128      # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch strain_dataloaderize, learning rate should change by the same factor to have comparable results\n",
        "LR = 2               # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 1e-4  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 70             # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = [49,63]      # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.2                 # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10\n",
        "\n",
        "CLASSES_PER_GROUP=10\n",
        "NUM_GROUPS=10\n",
        "NUM_EXEMPLARS=2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuVdgbGr-Y_C",
        "colab_type": "text"
      },
      "source": [
        "**Utils functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7hyAp8bJI8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(val_dataloader,net,icarl,conf_matrix=False):\n",
        "    net.train(False)\n",
        "    running_corrects = 0\n",
        "    y_pred = []\n",
        "    all_labels = []\n",
        "    for images, labels,_ in val_dataloader:\n",
        "\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # Forward Pass\n",
        "        preds = icarl.predict(images,net)\n",
        "        # Update Corrects\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "        y_pred += list(map(lambda x : x.item(),preds))\n",
        "        all_labels += list(labels)\n",
        "\n",
        "        # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    if(conf_matrix == True):\n",
        "        all_labels = list(map(lambda label : label.item(),all_labels))\n",
        "        return accuracy,confusion_matrix(y_pred,np.array(all_labels))\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDvEQernpNj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_loaders(train_dataloader):\n",
        "    all_labels = []\n",
        "    for images,labels,_ in train_dataloader:\n",
        "      all_labels += list(map(lambda x: x.item(),labels))\n",
        "\n",
        "    print(np.unique(all_labels,return_counts=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kitbSfdqhd9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def after_training(step,n_old_classes,train_dataloader,icarl,net,utils,training_set,type_prediction = 'NME',type_reduction = 'random'):\n",
        "  \n",
        "  images_indices = utils.create_images_indices(train_dataloader,step)\n",
        "  centroids = icarl.compute_centroids(net,training_set,images_indices,n_old_classes)\n",
        "\n",
        "  if len(icarl.exemplar_set) > 0:\n",
        "    print(\"Reducing the exemplar set..\")\n",
        "    icarl.reduce_exemplars(n_old_classes)\n",
        "  \n",
        "  print(\"Building the exemplar set...\")\n",
        "  if type_reduction == 'random':\n",
        "    icarl.build_exemplars_random(images_indices,n_old_classes)\n",
        "  elif type_reduction == 'herding':\n",
        "    icarl.build_exemplars_herding(net,images_indices,n_old_classes)\n",
        "\n",
        "  return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtaJ1cyq-dQJ",
        "colab_type": "text"
      },
      "source": [
        "**Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbJtQL1hJagQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myNet = MyNet(n_classes=CLASSES_PER_GROUP)\n",
        "icarl = Icarl(K=NUM_EXEMPLARS)\n",
        "utils = Utils()\n",
        "myLoss = Loss()\n",
        "typeScheduler='multistep' # In this case it can be only set to multistep\n",
        "\n",
        "#Creating dataloader for the first group of 10 classes\n",
        "train_dataloader_exemplars,test_dataloader = utils.create_dataloaders_icarl(training_set,test_set,1,icarl.exemplar_set,BATCH_SIZE)\n",
        "\n",
        "old_outputs=[]\n",
        "\n",
        "for i in range(NUM_GROUPS):\n",
        "    step=i+1 \n",
        "    print(\"STARTING MM TRAINING WITH GROUP:\\t\",step)  \n",
        "\n",
        "    n_old_classes = CLASSES_PER_GROUP*(step-1)\n",
        "    if step > 1:\n",
        "      WEIGHT_DECAY = WEIGHT_DECAY * ((step-1)/(step*step - 2)) \n",
        "      myNet.update_network(myNet.net,CLASSES_PER_GROUP + n_old_classes,myNet.init_weights)\n",
        "      train_dataloader_exemplars,test_dataloader = utils.create_dataloaders_icarl(training_set,test_set,step,icarl.exemplar_set,BATCH_SIZE)\n",
        "      test_loaders(train_dataloader_exemplars)\n",
        "    \n",
        "    optimizer,scheduler = myNet.prepare_training(LR,MOMENTUM,WEIGHT_DECAY,STEP_SIZE,GAMMA,typeScheduler=typeScheduler)\n",
        "\n",
        "    classification_losses = []\n",
        "    distillation_losses = []\n",
        "\n",
        "    myNet.net.to(DEVICE)\n",
        "    cudnn.benchmark \n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        running_correct_train = 0\n",
        "        if typeScheduler == 'multistep':\n",
        "          print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr()))\n",
        "\n",
        "        myNet.net.train() # Set Network to train mode\n",
        "        current_step = 0\n",
        "        for images, labels, _ in train_dataloader_exemplars:\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            \n",
        "            #Set all gradients to zero\n",
        "            optimizer.zero_grad() \n",
        "\n",
        "            #Computing output and creating the acyclic graph for updating the gradients\n",
        "            outputs = myNet.net(images) \n",
        "\n",
        "            #Computing predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            #Get predictions of the previous net\n",
        "            if(step > 1):\n",
        "                old_outputs = myNet.get_old_outputs(images,labels)\n",
        "\n",
        "            #Computing loss\n",
        "            loss,clf_loss,dist_loss = myLoss.MMLoss_onlydist_Prob(old_outputs,outputs,labels,step,current_step,utils,CLASSES_PER_GROUP)\n",
        "            classification_losses.append(clf_loss.item())\n",
        "            distillation_losses.append(dist_loss.item())\n",
        "            \n",
        "            #Calculate correct predictions\n",
        "            running_correct_train += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            #Accumulate gradients\n",
        "            loss.backward()  \n",
        "\n",
        "            # Update weights based on accumulated gradients  \n",
        "            optimizer.step()\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        #Calculate training accuracy\n",
        "        train_accuracy = running_correct_train/len(train_dataloader_exemplars.dataset)\n",
        "        print(\"Accuracy on the training :\\t\",train_accuracy)\n",
        "\n",
        "        if typeScheduler == 'multistep':\n",
        "            scheduler.step()\n",
        "\n",
        "    #Handling Exemplars\n",
        "    exemplar_dataloader = DataLoader(training_set.get_group(step),batch_size=BATCH_SIZE,drop_last=False,num_workers=4)\n",
        "    after_training(step,n_old_classes,exemplar_dataloader,icarl,myNet.net,utils,training_set)\n",
        "\n",
        "    #Test\n",
        "    test_accuracy,test_matrix = validation(test_dataloader,myNet.net,icarl,conf_matrix=True)\n",
        "    print(\"Accuracy on the test :\\t\",test_accuracy)\n",
        "\n",
        "    #Writing on file    \n",
        "    utils.writeOnFileMetrics('iCaRLMetrics.json', step, [train_accuracy,None,test_accuracy,test_matrix.tolist()])\n",
        "    utils.writeOnFileLosses('iCaRLLosses.json', step, [classification_losses,distillation_losses])\n",
        "    !cp  './iCaRLMetrics.json' './gdrive/My Drive/iCaRLMetrics.json'\n",
        "    !cp  'iCaRLLosses.json' './gdrive/My Drive/iCaRLLosses.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jysikUeoATxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1bZs3iX7_Zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m=torch.Tensor([[0,0,0,0],[0,0,0,1],[0,0,0,0],[0,0,0,0]])\n",
        "print(m)\n",
        "\n",
        "n=torch.sum(m, dim=-1, keepdim=True)\n",
        "print(n)\n",
        "m= torch.zeros(m.shape)+n\n",
        "print(m)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}