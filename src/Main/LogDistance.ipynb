{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogDistance.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTlj0v4qrzFT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "de422c31-fa8f-4159-8dfe-a984101e0fc5"
      },
      "source": [
        "!rm -r 'IncrementalLeraningMLDL'\n",
        "!git clone \"https://github.com/wAnto97/IncrementalLeraningMLDL\"\n",
        "from IncrementalLeraningMLDL.src.CIFAR100_dataset import MyCIFAR100\n",
        "from IncrementalLeraningMLDL.src.Utils import Utils\n",
        "from IncrementalLeraningMLDL.src.MyNet import MyNet\n",
        "from IncrementalLeraningMLDL.src.Icarl import Icarl\n",
        "from IncrementalLeraningMLDL.src.Loss import Loss\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "import copy\n",
        "from torch.backends import cudnn\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import  DataLoader\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'IncrementalLeraningMLDL'...\n",
            "remote: Enumerating objects: 236, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/236)\u001b[K\rremote: Counting objects:   1% (3/236)\u001b[K\rremote: Counting objects:   2% (5/236)\u001b[K\rremote: Counting objects:   3% (8/236)\u001b[K\rremote: Counting objects:   4% (10/236)\u001b[K\rremote: Counting objects:   5% (12/236)\u001b[K\rremote: Counting objects:   6% (15/236)\u001b[K\rremote: Counting objects:   7% (17/236)\u001b[K\rremote: Counting objects:   8% (19/236)\u001b[K\rremote: Counting objects:   9% (22/236)\u001b[K\rremote: Counting objects:  10% (24/236)\u001b[K\rremote: Counting objects:  11% (26/236)\u001b[K\rremote: Counting objects:  12% (29/236)\u001b[K\rremote: Counting objects:  13% (31/236)\u001b[K\rremote: Counting objects:  14% (34/236)\u001b[K\rremote: Counting objects:  15% (36/236)\u001b[K\rremote: Counting objects:  16% (38/236)\u001b[K\rremote: Counting objects:  17% (41/236)\u001b[K\rremote: Counting objects:  18% (43/236)\u001b[K\rremote: Counting objects:  19% (45/236)\u001b[K\rremote: Counting objects:  20% (48/236)\u001b[K\rremote: Counting objects:  21% (50/236)\u001b[K\rremote: Counting objects:  22% (52/236)\u001b[K\rremote: Counting objects:  23% (55/236)\u001b[K\rremote: Counting objects:  24% (57/236)\u001b[K\rremote: Counting objects:  25% (59/236)\u001b[K\rremote: Counting objects:  26% (62/236)\u001b[K\rremote: Counting objects:  27% (64/236)\u001b[K\rremote: Counting objects:  28% (67/236)\u001b[K\rremote: Counting objects:  29% (69/236)\u001b[K\rremote: Counting objects:  30% (71/236)\u001b[K\rremote: Counting objects:  31% (74/236)\u001b[K\rremote: Counting objects:  32% (76/236)\u001b[K\rremote: Counting objects:  33% (78/236)\u001b[K\rremote: Counting objects:  34% (81/236)\u001b[K\rremote: Counting objects:  35% (83/236)\u001b[K\rremote: Counting objects:  36% (85/236)\u001b[K\rremote: Counting objects:  37% (88/236)\u001b[K\rremote: Counting objects:  38% (90/236)\u001b[K\rremote: Counting objects:  39% (93/236)\u001b[K\rremote: Counting objects:  40% (95/236)\u001b[K\rremote: Counting objects:  41% (97/236)\u001b[K\rremote: Counting objects:  42% (100/236)\u001b[K\rremote: Counting objects:  43% (102/236)\u001b[K\rremote: Counting objects:  44% (104/236)\u001b[K\rremote: Counting objects:  45% (107/236)\u001b[K\rremote: Counting objects:  46% (109/236)\u001b[K\rremote: Counting objects:  47% (111/236)\u001b[K\rremote: Counting objects:  48% (114/236)\u001b[K\rremote: Counting objects:  49% (116/236)\u001b[K\rremote: Counting objects:  50% (118/236)\u001b[K\rremote: Counting objects:  51% (121/236)\u001b[K\rremote: Counting objects:  52% (123/236)\u001b[K\rremote: Counting objects:  53% (126/236)\u001b[K\rremote: Counting objects:  54% (128/236)\u001b[K\rremote: Counting objects:  55% (130/236)\u001b[K\rremote: Counting objects:  56% (133/236)\u001b[K\rremote: Counting objects:  57% (135/236)\u001b[K\rremote: Counting objects:  58% (137/236)\u001b[K\rremote: Counting objects:  59% (140/236)\u001b[K\rremote: Counting objects:  60% (142/236)\u001b[K\rremote: Counting objects:  61% (144/236)\u001b[K\rremote: Counting objects:  62% (147/236)\u001b[K\rremote: Counting objects:  63% (149/236)\u001b[K\rremote: Counting objects:  64% (152/236)\u001b[K\rremote: Counting objects:  65% (154/236)\u001b[K\rremote: Counting objects:  66% (156/236)\u001b[K\rremote: Counting objects:  67% (159/236)\u001b[K\rremote: Counting objects:  68% (161/236)\u001b[K\rremote: Counting objects:  69% (163/236)\u001b[K\rremote: Counting objects:  70% (166/236)\u001b[K\rremote: Counting objects:  71% (168/236)\u001b[K\rremote: Counting objects:  72% (170/236)\u001b[K\rremote: Counting objects:  73% (173/236)\u001b[K\rremote: Counting objects:  74% (175/236)\u001b[K\rremote: Counting objects:  75% (177/236)\u001b[K\rremote: Counting objects:  76% (180/236)\u001b[K\rremote: Counting objects:  77% (182/236)\u001b[K\rremote: Counting objects:  78% (185/236)\u001b[K\rremote: Counting objects:  79% (187/236)\u001b[K\rremote: Counting objects:  80% (189/236)\u001b[K\rremote: Counting objects:  81% (192/236)\u001b[K\rremote: Counting objects:  82% (194/236)\u001b[K\rremote: Counting objects:  83% (196/236)\u001b[K\rremote: Counting objects:  84% (199/236)\u001b[K\rremote: Counting objects:  85% (201/236)\u001b[K\rremote: Counting objects:  86% (203/236)\u001b[K\rremote: Counting objects:  87% (206/236)\u001b[K\rremote: Counting objects:  88% (208/236)\u001b[K\rremote: Counting objects:  89% (211/236)\u001b[K\rremote: Counting objects:  90% (213/236)\u001b[K\rremote: Counting objects:  91% (215/236)\u001b[K\rremote: Counting objects:  92% (218/236)\u001b[K\rremote: Counting objects:  93% (220/236)\u001b[K\rremote: Counting objects:  94% (222/236)\u001b[K\rremote: Counting objects:  95% (225/236)\u001b[K\rremote: Counting objects:  96% (227/236)\u001b[K\rremote: Counting objects:  97% (229/236)\u001b[K\rremote: Counting objects:  98% (232/236)\u001b[K\rremote: Counting objects:  99% (234/236)\u001b[K\rremote: Counting objects: 100% (236/236)\u001b[K\rremote: Counting objects: 100% (236/236), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 1420 (delta 163), reused 161 (delta 90), pack-reused 1184\u001b[K\n",
            "Receiving objects: 100% (1420/1420), 10.93 MiB | 21.86 MiB/s, done.\n",
            "Resolving deltas: 100% (964/964), done.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jbSB5id_PC9",
        "colab_type": "text"
      },
      "source": [
        "**Loading data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NtjfRgDVse6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "13b66087-c056-43e6-dcf6-12765a66d057"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "                                      transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize( (0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))]) # Normalizes tensor with mean and standard deviation\n",
        "\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize( (0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "training_set = MyCIFAR100('/content',train=True, n_groups=10, transform=train_transform, download=True,random_state = 653)\n",
        "test_set = MyCIFAR100('/content',train=False, n_groups=10, transform=eval_transform, download=True,random_state = 653)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAub61CI_RQa",
        "colab_type": "text"
      },
      "source": [
        "**Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-do-BtKUI4F-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'  \n",
        "\n",
        "BATCH_SIZE = 128      # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch strain_dataloaderize, learning rate should change by the same factor to have comparable results\n",
        "LR = 2              # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 1e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 70             # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = [49,63]      # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.2                 # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10\n",
        "\n",
        "CLASSES_PER_GROUP=10\n",
        "NUM_GROUPS=10\n",
        "NUM_EXEMPLARS=2000"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuVdgbGr-Y_C",
        "colab_type": "text"
      },
      "source": [
        "**Utils functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7hyAp8bJI8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(val_dataloader,net,icarl,conf_matrix=False):\n",
        "    net.train(False)\n",
        "    running_corrects = 0\n",
        "    y_pred = []\n",
        "    all_labels = []\n",
        "    for images, labels,_ in val_dataloader:\n",
        "\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # Forward Pass\n",
        "        preds = icarl.predict(images,net)\n",
        "\n",
        "        # Update Corrects\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "        y_pred += list(map(lambda x : x.item(),preds))\n",
        "        all_labels += list(labels)\n",
        "\n",
        "        # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    if(conf_matrix == True):\n",
        "        all_labels = list(map(lambda label : label.item(),all_labels))\n",
        "        return accuracy,confusion_matrix(y_pred,np.array(all_labels))\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDvEQernpNj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_loaders(train_dataloader):\n",
        "    all_labels = []\n",
        "    for images,labels,_ in train_dataloader:\n",
        "      all_labels += list(map(lambda x: x.item(),labels))\n",
        "\n",
        "    print(np.unique(all_labels,return_counts=True))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kitbSfdqhd9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def after_training(step,n_old_classes,train_dataloader,icarl,net,utils,training_set,type_prediction = 'NME',type_reduction = 'random'):\n",
        "  \n",
        "  images_indices = utils.create_images_indices(train_dataloader,step)\n",
        "  centroids = icarl.compute_centroids(net,training_set,images_indices,n_old_classes)\n",
        "\n",
        "  if len(icarl.exemplar_set) > 0:\n",
        "    print(\"Reducing the exemplar set..\")\n",
        "    icarl.reduce_exemplars(n_old_classes)\n",
        "  \n",
        "  print(\"Building the exemplar set...\")\n",
        "  if type_reduction == 'random':\n",
        "    icarl.build_exemplars_random(images_indices,n_old_classes)\n",
        "  elif type_reduction == 'herding':\n",
        "    icarl.build_exemplars_herding(net,images_indices,n_old_classes)\n",
        "\n",
        "  return "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtaJ1cyq-dQJ",
        "colab_type": "text"
      },
      "source": [
        "**Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbJtQL1hJagQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "10e7b81b-0e18-450b-a0db-8c688520d4fe"
      },
      "source": [
        "myNet = MyNet(n_classes=CLASSES_PER_GROUP)\n",
        "icarl = Icarl(K=NUM_EXEMPLARS)\n",
        "utils = Utils()\n",
        "myLoss = Loss()\n",
        "typeScheduler='multistep' # In this case it can be only set to multistep\n",
        "\n",
        "#Creating dataloader for the first group of 10 classes\n",
        "train_dataloader_exemplars,test_dataloader = utils.create_dataloaders_icarl(training_set,test_set,1,icarl.exemplar_set,BATCH_SIZE)\n",
        "\n",
        "old_outputs=[]\n",
        "\n",
        "for i in range(NUM_GROUPS):\n",
        "    step=i+1 \n",
        "    print(\"STARTING LogDistance TRAINING WITH GROUP:\\t\",step)  \n",
        "\n",
        "    n_old_classes = CLASSES_PER_GROUP*(step-1)\n",
        "    if step > 1:\n",
        "      myNet.update_network(myNet.net,CLASSES_PER_GROUP + n_old_classes,myNet.init_weights)\n",
        "      train_dataloader_exemplars,test_dataloader = utils.create_dataloaders_icarl(training_set,test_set,step,icarl.exemplar_set,BATCH_SIZE)\n",
        "      test_loaders(train_dataloader_exemplars)\n",
        "    \n",
        "    optimizer,scheduler = myNet.prepare_training(LR,MOMENTUM,WEIGHT_DECAY,STEP_SIZE,GAMMA,typeScheduler=typeScheduler)\n",
        "\n",
        "    classification_losses = []\n",
        "    distillation_losses = []\n",
        "\n",
        "    myNet.net.to(DEVICE)\n",
        "    cudnn.benchmark \n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        running_correct_train = 0\n",
        "        if typeScheduler == 'multistep':\n",
        "          print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_last_lr()))\n",
        "\n",
        "        myNet.net.train() # Set Network to train mode\n",
        "        current_step = 0\n",
        "        for images, labels, _ in train_dataloader_exemplars:\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            \n",
        "            #Set all gradients to zero\n",
        "            optimizer.zero_grad() \n",
        "\n",
        "            #Computing output and creating the acyclic graph for updating the gradients\n",
        "            outputs = myNet.net(images) \n",
        "\n",
        "            #Computing predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            \n",
        "            #Get predictions of the previous net\n",
        "            if(step > 1):\n",
        "                old_outputs = myNet.get_old_outputs(images,labels)\n",
        "\n",
        "            #Computing loss\n",
        "            loss,clf_loss,dist_loss = myLoss.abs_log_loss(old_outputs,outputs,labels,step,current_step,utils,CLASSES_PER_GROUP)\n",
        "            classification_losses.append(clf_loss.item())\n",
        "            distillation_losses.append(dist_loss.item())\n",
        "            \n",
        "            #Calculate correct predictions\n",
        "            running_correct_train += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            #Accumulate gradients\n",
        "            loss.backward()  \n",
        "\n",
        "            # Update weights based on accumulated gradients  \n",
        "            optimizer.step()\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        #Calculate training accuracy\n",
        "        train_accuracy = running_correct_train/len(train_dataloader_exemplars.dataset)\n",
        "        print(\"Accuracy on the training :\\t\",train_accuracy)\n",
        "\n",
        "        if typeScheduler == 'multistep':\n",
        "            scheduler.step()\n",
        "\n",
        "    #Handling Exemplars\n",
        "    exemplar_dataloader = DataLoader(training_set.get_group(step),batch_size=BATCH_SIZE,drop_last=False,num_workers=4)\n",
        "    after_training(step,n_old_classes,exemplar_dataloader,icarl,myNet.net,utils,training_set)\n",
        "\n",
        "    #Test\n",
        "    test_accuracy,test_matrix = validation(test_dataloader,myNet.net,icarl,conf_matrix=True)\n",
        "    print(\"Accuracy on the test :\\t\",test_accuracy)\n",
        "\n",
        "    #Writing on file    \n",
        "    utils.writeOnFileMetrics('LogDistanceMetrics.json', step, [train_accuracy,None,test_accuracy,test_matrix.tolist()])\n",
        "    utils.writeOnFileLosses('LogDistanceLosses.json', step, [classification_losses,distillation_losses])\n",
        "    !cp  './LogDistanceMetrics.json' './gdrive/My Drive/LogDistanceMetrics.json'\n",
        "    !cp  'LogDistanceLosses.json' './gdrive/My Drive/LogDistanceLosses.json'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STARTING ICARL TRAINING WITH GROUP:\t 1\n",
            "Starting epoch 1/70, LR = [2]\n",
            "Accuracy on the training :\t 0.0992\n",
            "Starting epoch 2/70, LR = [2]\n",
            "Accuracy on the training :\t 0.1174\n",
            "Starting epoch 3/70, LR = [2]\n",
            "Accuracy on the training :\t 0.1174\n",
            "Starting epoch 4/70, LR = [2]\n",
            "Accuracy on the training :\t 0.1174\n",
            "Starting epoch 5/70, LR = [2]\n",
            "Accuracy on the training :\t 0.1264\n",
            "Starting epoch 6/70, LR = [2]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}